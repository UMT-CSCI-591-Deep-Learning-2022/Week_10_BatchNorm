{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_10_BatchNorm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Week 10: Batch Normalization\n",
        "\n",
        "This week we discussed batch normalization where inputs to hidden layers in the model are normalized to provide a speed-up in training. Here we will be training a simple MNIST classifier and comparing a plain network to one with batch normalization to demonstrate the training speed-up and visualize some of the effects of BatchNorm on the neural network during training."
      ],
      "metadata": {
        "id": "nBho30pNXanU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P_9rASz-yxG"
      },
      "outputs": [],
      "source": [
        "# Load the libraries and mnist data\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import seaborn as sns  # might need to install this one first\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "\n",
        "%matplotlib notebook\n",
        "\n",
        "if(\"dataset\" not in globals()):\n",
        "    root_dir = Path().resolve()\n",
        "    dataset = torchvision.datasets.MNIST(root_dir, download=True, transform=transforms.ToTensor())\n",
        "    trainset, testset = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.8), int(len(dataset) * 0.2)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what some of our data looks like by plotting some of the MNIST images and the distribution of their pixels."
      ],
      "metadata": {
        "id": "eBAv-_2g6BiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot some images\n",
        "%matplotlib inline\n",
        "img1 = dataset[0][0]\n",
        "print(\"Image1 Size:\", tuple(img1.shape))\n",
        "new_img1 = img1.reshape(28,28)\n",
        "\n",
        "img2 = dataset[4][0]\n",
        "new_img2 = img2.reshape(28,28)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(new_img1)\n",
        "plt.figure()\n",
        "plt.imshow(new_img2)"
      ],
      "metadata": {
        "id": "SOVFFmY-z2iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert image to numpy array\n",
        "img_np1 = np.array(new_img1) \n",
        "# plot the pixel values\n",
        "plt.figure()\n",
        "plt.hist(img_np1.ravel(), bins=50, density=True)\n",
        "plt.xlabel(\"pixel values\")\n",
        "plt.ylabel(\"relative frequency\")\n",
        "plt.title(\"distribution of pixels (img1)\")\n",
        "\n",
        "# convert image to numpy array\n",
        "img_np2 = np.array(new_img2) \n",
        "# plot the pixel values\n",
        "plt.figure()\n",
        "plt.hist(img_np2.ravel(), bins=50, density=True)\n",
        "plt.xlabel(\"pixel values\")\n",
        "plt.ylabel(\"relative frequency\")\n",
        "plt.title(\"distribution of pixels (img2)\")"
      ],
      "metadata": {
        "id": "5Iju9MsO4HUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\") # GPU\n",
        "else:\n",
        "    device = torch.device(\"cpu\") # CPU\n",
        "    \n",
        "print(f\"Running PyTorch Using: {device}\")"
      ],
      "metadata": {
        "id": "iMuKRQgmh7gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a test and training set\n",
        "batch_size = 60\n",
        "\n",
        "to_device = lambda a: a.to(device)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size, shuffle=True)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "PHQavjSRiH66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used later to view hidden layer activations\n",
        "class ActivationTracker(nn.Module):\n",
        "    '''keeps track of the current activation'''\n",
        "    def __init__(self):\n",
        "        super(ActivationTracker, self).__init__()\n",
        "\n",
        "        # Keep track of [0.15, 0.5, 0.85] percentiles\n",
        "        self.percents_activation_track = torch.tensor([0.15, 0.50, 0.85]).to(device)\n",
        "        self.all_percents_activation = []\n",
        "\n",
        "    def get_all_activations(self):\n",
        "        return np.array(self.all_percents_activation)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # track activations of the first node\n",
        "        percents_activation = torch.quantile(x.detach()[:,0], self.percents_activation_track)\n",
        "        self.all_percents_activation.append(percents_activation.cpu().detach().numpy())\n",
        "        return x"
      ],
      "metadata": {
        "id": "D7bZeZMn3ej0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize linear layer weights with a gaussian as done in the paper\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.normal_(m.weight,0, 0.1)\n",
        "        m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "H9CGNwuOe1-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a basic network\n",
        "class VanillaNN(nn.Module):\n",
        "    def __init__(self): \n",
        "        super(VanillaNN, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            # TODO: what should the input dim be? Hint: size of img data\n",
        "            nn.Linear(None, 48),  # layer 1\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, 24),  # layer 2\n",
        "            ActivationTracker(),\n",
        "            nn.ReLU(),\n",
        "            # TODO: output dim\n",
        "            nn.Linear(24, None)  # output\n",
        "        )\n",
        "        \n",
        "        #option to use the architecture from the paper instead, but it's slower\n",
        "        # self.classifier = nn.Sequential(\n",
        "        #     nn.Linear(None,100),\n",
        "        #     nn.Sigmoid(),\n",
        "        #     nn.Linear(100,100),\n",
        "        #     nn.Sigmoid(),\n",
        "        #     nn.Linear(100,100),\n",
        "        #     ActivationTracker(),\n",
        "        #     nn.Sigmoid(),\n",
        "        #     nn.Linear(100,None)\n",
        "        # )\n",
        "        # self.classifier.apply(init_weights)\n",
        "             \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def get_model(self):\n",
        "      return self.classifier"
      ],
      "metadata": {
        "id": "HBhxT9cIinub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the same structure of the basic network as above, add batch normalization to the input layers\n",
        "class BatchNormNN(nn.Module):\n",
        "    def __init__(self): \n",
        "        super(BatchNormNN, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            #TODO: what should the input dim be? Hint: size of img data\n",
        "            nn.Linear(None, 48),   # layer 1\n",
        "            # TODO: Add batch norm layers before each activation -> nn.BatchNorm1d(size)\n",
        "            \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, 24),  # layer 2\n",
        "            \n",
        "            ActivationTracker(),\n",
        "            nn.ReLU(),\n",
        "            # TODO: output dim\n",
        "            nn.Linear(24, None)  # output\n",
        "        )\n",
        "        \n",
        "        #option to use the architecture from the paper instead, but it's slower\n",
        "        # self.classifier = nn.Sequential(\n",
        "        #     nn.Linear(None,100),\n",
        "        #     \n",
        "        #     nn.Sigmoid(),\n",
        "        #     nn.Linear(100,100),\n",
        "        #     \n",
        "        #     nn.Sigmoid(),\n",
        "        #     nn.Linear(100,100),\n",
        "        #     \n",
        "        #     ActivationTracker(),\n",
        "        #     nn.Sigmoid(),\n",
        "        #     nn.Linear(100,None)\n",
        "        # )\n",
        "        # self.classifier.apply(init_weights)\n",
        "             \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def get_model(self):\n",
        "      return self.classifier"
      ],
      "metadata": {
        "id": "J9HCBQlnjCDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to get the accuracy of a given model\n",
        "def get_accuracy(model, data):\n",
        "        run = 0\n",
        "        correct = 0\n",
        "        \n",
        "        for img, label in data:\n",
        "            run += len(img)\n",
        "            result = model.forward(img.to(device)).cpu().detach().numpy()\n",
        "            correct += np.sum(np.argmax(result, axis=1) == label.cpu().detach().numpy())\n",
        "        \n",
        "        return correct / run"
      ],
      "metadata": {
        "id": "2cd_O7uWEMCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our data and models defined, let's train both models using the same parameters and compare their loss over time."
      ],
      "metadata": {
        "id": "OKHS_q39ZEt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start here to train the models\n",
        "model = VanillaNN().to(device)\n",
        "print(model)\n",
        "model_bn = BatchNormNN().to(device)\n",
        "print(model_bn)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(model.parameters(), lr=0.01)\n",
        "opt_bn = optim.SGD(model_bn.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "ylqlVBZREYC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_arr = []\n",
        "loss_bn_arr = []\n",
        "epochs_stats = []\n",
        "\n",
        "# TODO: set number of epochs\n",
        "max_epochs = None\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "        inputs, labels = data\n",
        "        \n",
        "        \n",
        "\n",
        "        # training steps for normal model\n",
        "        # print(\"train\")\n",
        "        # activations get added here so track iteration\n",
        "        epochs_stats.append(i+epoch*len(trainloader))\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = loss_fn(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        \n",
        "        # training steps for bn model\n",
        "        opt_bn.zero_grad()\n",
        "        outputs_bn = model_bn(inputs.to(device))\n",
        "        loss_bn = loss_fn(outputs_bn, labels.to(device))\n",
        "        loss_bn.backward()\n",
        "        opt_bn.step()\n",
        "        \n",
        "        loss_arr.append(loss.item())\n",
        "        loss_bn_arr.append(loss_bn.item())\n",
        "\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            inputs = inputs.view(inputs.size(0), -1)\n",
        "            model.eval()\n",
        "            model_bn.eval()\n",
        "            \n",
        "            # plot input layer distributions\n",
        "            # first linear layer\n",
        "            a = model.classifier[0](inputs.to(device))\n",
        "            # inputs to first layer, first node\n",
        "            a = a[:,0].cpu().detach().numpy().ravel()\n",
        "            sns.distplot(a, kde=True, color='r', label='Normal') \n",
        "            \n",
        "            # first linear layer, batch layer\n",
        "            b = model_bn.classifier[0](inputs.to(device))\n",
        "            b = model_bn.classifier[1](b)\n",
        "            # inputs to first layer, first node\n",
        "            b = b[:,0].cpu().detach().numpy().ravel()\n",
        "            \n",
        "            sns.distplot(b, kde=True, color='g', label='BatchNorm') \n",
        "            plt.title('%d: Loss = %0.2f, Loss with BN = %0.2f' % (i, loss.item(), loss_bn.item()))\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "            plt.pause(0.5)\n",
        "            model.train()\n",
        "            model_bn.train()\n",
        "        \n",
        "    print('----------------------')\n",
        "    print(epoch)\n",
        "    plt.plot(loss_arr, 'r', label='Normal')\n",
        "    plt.plot(loss_bn_arr, 'g', label='BatchNorm')\n",
        "    plt.xlabel(\"Batch Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# loss of the network with batch normalization reduces much faster than the normal network"
      ],
      "metadata": {
        "id": "jy_JzBjYj2KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There were quite a few visualizations produced while training the models.\n",
        "We have density plots which show the distribution of the inputs to the first activation for the first node for each model and the loss of the models throughout batch iterations. \n",
        "\n",
        "**TODO: what are we seeing from the density plots? Think about the pixel value distributions of some of the MNIST data at the beginning, would this influence the output of our plots?**\n",
        "\n",
        "**TODO: compare the loss over time from the vanilla model with the batch norm model. What can we infer from this?**"
      ],
      "metadata": {
        "id": "KtwB2Qq-2GbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get last hidden layer activation (ActivationTracker layer)\n",
        "# TODO: which layer of the model to use?\n",
        "baseline_activation = model.get_model()[None].get_all_activations()\n",
        "bn_activation = model_bn.get_model()[None].get_all_activations()\n",
        "print(len(baseline_activation))  # will match len(epochs_stats)"
      ],
      "metadata": {
        "id": "l95royrcJb0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper, they show how the activations from the last layer change during training and compare the Vanilla network to the BatchNorm network (Figure 1(b) and 1(c)).\n",
        "\n",
        "**TODO: What effect would we expect BatchNorm to have on these last layer activations?**"
      ],
      "metadata": {
        "id": "GQRq9s3dr326"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display activations\n",
        "if epochs_stats: \n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10*2, 5))\n",
        "\n",
        "    # -- All iterations\n",
        "    # Baseline\n",
        "    ax[0].plot(epochs_stats, baseline_activation[:, 0], label = f\"0.15 percentile\")\n",
        "    ax[0].plot(epochs_stats, baseline_activation[:, 1], label = f\"0.50 percentile\")\n",
        "    ax[0].plot(epochs_stats, baseline_activation[:, 2], label = f\"0.85 percentile\")\n",
        "    ax[0].set_title('Without BN')\n",
        "    ax[0].set_xlabel('Iterations')\n",
        "    ax[0].set_ylabel('Activation for one node')\n",
        "    \n",
        "    # BN based\n",
        "    ax[1].plot(epochs_stats, bn_activation[:, 0], label = f\"0.15 percentile\")\n",
        "    ax[1].plot(epochs_stats, bn_activation[:, 1], label = f\"0.50 percentile\")\n",
        "    ax[1].plot(epochs_stats, bn_activation[:, 2], label = f\"0.85 percentile\")\n",
        "    ax[1].set_title('With BN')\n",
        "    ax[1].set_xlabel('Iterations')\n",
        "    ax[1].set_ylabel('Activation for one node')\n",
        "    \n",
        "    ax[0].legend(); ax[1].legend()\n",
        "    plt.suptitle('Last hidden layer activation, single node', fontsize=20)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "j7sxK06fJjBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO: How do your plots compare to those in the paper? How do they compare to each other? What might explain the differences that you're seeing?**\n",
        "\n",
        "**TODO: If you make changes to the architecture or training regime, how does this affect the behavior of these last activation values?**"
      ],
      "metadata": {
        "id": "jFC7yndTtw9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --Zoom into beginning of above plots to visualize smoothing effect\n",
        "small_window = 500\n",
        "\n",
        "if epochs_stats: \n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10*2, 5))\n",
        "\n",
        "    # Baseline\n",
        "    ax[0].plot(epochs_stats[:small_window], baseline_activation[:small_window, 1], label = f\"0.50 percentile\")\n",
        "    ax[0].set_title('Without BN')\n",
        "    ax[0].set_xlabel('Iterations')\n",
        "    ax[0].set_ylabel('Activation (mean over the testset)')\n",
        "    \n",
        "    # BN based\n",
        "    ax[1].plot(epochs_stats[:small_window], bn_activation[:small_window, 1], label = f\"0.50 percentile\")\n",
        "    ax[1].set_title('With BN')\n",
        "    ax[1].set_xlabel('Iterations')\n",
        "    ax[1].set_ylabel('Activation (mean over the testset)')\n",
        "    \n",
        "    ax[0].legend(); ax[1].legend()\n",
        "    plt.suptitle('BN effect on activation smoothness', fontsize=20)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5kXWmQ9ZUb07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate accuracy of the models\n",
        "# vanilla accuracy\n",
        "acc1 = get_accuracy(model, trainloader)\n",
        "print(f\"Epoch {epoch} Vanilla Train Accuracy: {acc1 * 100:.02f}%\")\n",
        "acc2 = get_accuracy(model, testloader)\n",
        "print(f\"Epoch {epoch} Vanilla Test Accuracy: {acc2 * 100:.02f}%\\n\")\n",
        "\n",
        "# batch normalization accuracy\n",
        "acc1 = get_accuracy(model_bn, trainloader)\n",
        "print(f\"Epoch {epoch} BN Train Accuracy: {acc1 * 100:.02f}%\")\n",
        "acc2 = get_accuracy(model_bn, testloader)\n",
        "print(f\"Epoch {epoch} BN Test Accuracy: {acc2 * 100:.02f}%\\n\")"
      ],
      "metadata": {
        "id": "PKTW6bnzJTNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO: How does the accuracy of the models with and without BatchNorm compare? How does this change when you change the architecture, learning rate, or number of iterations?**"
      ],
      "metadata": {
        "id": "GRFCryntUf6G"
      }
    }
  ]
}